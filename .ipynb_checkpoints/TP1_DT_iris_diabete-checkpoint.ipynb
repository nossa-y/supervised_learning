{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <font color='green'> Première partie : Arbres de décision\n",
    "Dans cette première partie du TP, nous allons exploiter la base de données \"Iris de Fisher\" accessible via sklearn. \n",
    "Cette base fait appel à une variété de fleurs. Iris Fisher Database : https://en.wikipedia.org/wiki/Iris_flower_data_set\n",
    "\n",
    "\n",
    "##  <font color='blue'> Etape 1 : Visualiser la base \"Iris de Fisher\"\n",
    "\n",
    "- Déterminer les attributs, les classes et le nombre d'échantillons par classe\n",
    "- Quel type de données vous aurez à manipuler ?\n",
    "- Qu'observez-vous sur les attributs en termes de séparabilité des classes ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Reading the Iris.csv file\n",
    "iris = load_iris()\n",
    "\n",
    "# Print the label of species (classes)\n",
    "print(iris.target_names)\n",
    "\n",
    "# Print the names of the available features\n",
    "print(iris.feature_names)\n",
    "\n",
    "# Examples in each class \n",
    "print(\"Number of samples per class: %s\" % np.bincount(iris.target))\n",
    "\n",
    "# Print the iris labels (0:setosa, 1:versicolor, 2:virginica)\n",
    "print(iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame of the iris dataset\n",
    "data=pd.DataFrame({\n",
    "    'sepal length':iris.data[:,0],\n",
    "    'sepal width':iris.data[:,1],\n",
    "    'petal length':iris.data[:,2],\n",
    "    'petal width':iris.data[:,3],\n",
    "    'species':iris.target\n",
    "})\n",
    "data.head(10)\n",
    "# NB : (0:setosa, 1:versicolor, 2:virginica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Descriptive analysis of the database\n",
    "\n",
    "# Extract Attributes / Features\n",
    "X = iris.data\n",
    "\n",
    "# Extract Target / Class Labels\n",
    "y = iris.target\n",
    "\n",
    "#Some statistics on the data\n",
    "data.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the relashionship between two attributes\n",
    "\n",
    "#(0:setosa, 1:versicolor, 2:virginica)\n",
    "sns.set_style('whitegrid')\n",
    "sns.FacetGrid(data, hue='species', palette=\"viridis\", height=4, aspect=1.5)\\\n",
    "   .map(plt.scatter,'sepal length','sepal width')\\\n",
    "   .add_legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all pairs of attributes\n",
    "sns.set(style=\"ticks\", color_codes=False)\n",
    "g=sns.pairplot(data, hue='species', palette=\"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#   <font color='blue'>Etape 2. Classification supervisée avec les Arbres de Décision\n",
    "\n",
    "### Objectif :\n",
    "(i) Appliquer les arbres de décision sur cette base de données d'iris pour une classification multi-classes;\n",
    "\n",
    "(ii) Etudier l'impact des hyperparamètres sur les performances du classifieur. \n",
    "\n",
    "Pour cela :\n",
    "\n",
    "### 1. Construire un arbre de décision sur une base d'apprentissage (prendre 70% des exemples), en utilisant les hyperparamètres \"par défaut\"\n",
    "- Interpréter finement l'arbre de décision obtenu, et cela à chaque profondeur.\n",
    "- Que peut-on déduire sur la qualité de l'arbre ?\n",
    "- Utiliser l'arbre pour la prédiction de 4 fleurs (test). Montrer le processus de classification à travers les différentes couches de l'arbre (le chemin de décision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction de l'arbre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X=data[['sepal length', 'sepal width', 'petal length', 'petal width']]  # Features\n",
    "y=data['species']  # Labels\n",
    "\n",
    "# Create Train and Test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=0) #70% training and 30% test\n",
    "\n",
    "# Create Decision Tree Classifier on the training dataset\n",
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Visualize the tree after training\n",
    "plt.figure(figsize=(10,8)) \n",
    "tree.plot_tree(clf, class_names=iris.target_names, filled=True)\n",
    "[...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utiliser l'arbre pour l'inférence / la prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Predict Accuracy Score on test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Train data accuracy:\",round(100*accuracy_score(y_true = y_train, y_pred=clf.predict(X_train)),3),\"%\")\n",
    "print(\"Test data accuracy:\",\"%.3f\" %(100*accuracy_score(y_true = y_test, y_pred=y_pred)),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification of four new iris flowers\n",
    "\n",
    "idx1 = clf.predict([[5, 3, 1.4, 0.2]])[0]\n",
    "print(\"Flower 1:\", iris.target_names[idx1])\n",
    "\n",
    "species_idx2 = clf.predict([[3, 5, 4, 2]])[0]\n",
    "print(\"Flower 2:\", iris.target_names[species_idx2])\n",
    "\n",
    "species_idx3 = clf.predict([[6, 2, 0.4, 1.2]])[0]\n",
    "print(\"Flower 3:\", iris.target_names[species_idx3])\n",
    "\n",
    "species_idx4 = clf.predict([[6, 2, 0.4, 3]])[0]\n",
    "print(\"Flower 4:\", iris.target_names[species_idx4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report \n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "# Get the confusion matrix\n",
    "confusion_matrix = metrics.confusion_matrix(y_test,  \n",
    "                                            y_pred) # Turn this into a dataframe\n",
    "matrix_df = pd.DataFrame(confusion_matrix) # Plot the result\n",
    "ax = plt.axes()\n",
    "sns.set(font_scale=1.3)\n",
    "sns.heatmap(matrix_df, annot=True, fmt=\"g\", ax=ax, cmap=\"viridis\") # Set axis titles\n",
    "ax.set_title('Confusion Matrix - Iris classifcation')\n",
    "ax.set_xlabel(\"Predicted label\", fontsize =15)\n",
    "ax.set_ylabel(\"True Label\", fontsize=15) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Construire l'arbre de décision en fixant certains hyperparamètres\n",
    "\n",
    "- Appliquer différentes valeurs aux hyperparamètres de l'arbre : \"max_depth\", \"min_samples_split\", \"criterion\", etc. Puis, analyser leur impact sur les performances. Se référer à : https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "\n",
    "- Etudier l'impact de la prise en compte de 80%, 70%, 50% et 20% des données en apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can change the size of the training dataset 80%, 70%, 50%, 20%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=0) # 70% training and 30% test\n",
    "\n",
    "# Create Decision Tree Classifier by specifying some parametrs\n",
    "clf = tree.DecisionTreeClassifier(max_depth = 2, criterion=\"gini\")\n",
    "#clf = tree.DecisionTreeClassifier(max_leaf_nodes=2,max_depth = 3,min_samples_leaf = 20, criterion=\"entropy\", min_samples_split = 20)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Visualize the tree after training\n",
    "plt.figure(figsize=(8,6))\n",
    "tree.plot_tree(clf, class_names=iris.target_names, filled=True)\n",
    "[...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Accuracy Score\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Train data accuracy:\",\"%.3f\" %(100*accuracy_score(y_true = y_train, y_pred=clf.predict(X_train))),\"%\")\n",
    "print(\"Test data accuracy:\",\"%.3f\" %(100*accuracy_score(y_true = y_test, y_pred=y_pred)),\"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "N = 100\n",
    "accuracies = []\n",
    "for i in range(N):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    Z = clf.predict(X_test)\n",
    "    accuracies.append(clf.score(X_test,y_test))\n",
    "    \n",
    "pgrid = {\"max_depth\": [1, 5, 10, 20, 25, 30, 35],\n",
    "        \"min_samples_split\": [2, 5, 10, 15, 20]}\n",
    "accuracies.append(clf.score(X_test,y_test))\n",
    "grid_search = GridSearchCV(tree.DecisionTreeClassifier(), param_grid=pgrid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Meilleur score : \" + str(grid_search.best_estimator_.score(X_test, y_test)))\n",
    "print(\"max_depth: \" + str(grid_search.best_estimator_.max_depth))\n",
    "print(\"min_samples_split: \" + str(grid_search.best_estimator_.min_samples_split))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "#   <font color='blue'>Etape 3. Vers la détection du diabète \n",
    "\n",
    "## Classification binaire avec les arbres de décision\n",
    "\n",
    "Dans cette deuxième partie du TP, nous allons exploiter aussi la base de données traitant de la problématique de la détection du diabete, en traitant la base de données accessible via \"diabetes.csv\". \n",
    "\n",
    "- Décrire la base de données. \n",
    "- Présenter la méthodologie d'utilisation de l'arbre de décision dans ce problème de classification.\n",
    "- Interpréter les résultats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"diabetes.csv\")\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.sample(5, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xdiab = dataset.drop(columns='Outcome')\n",
    "ydiab = dataset['Outcome']\n",
    "\n",
    "Xdiab_train, Xdiab_test, ydiab_train, ydiab_test = train_test_split(Xdiab, ydiab, test_size=0.3, random_state=0)\n",
    "\n",
    "clfdiab = tree.DecisionTreeClassifier()\n",
    "clfdiab.fit(Xdiab_train, ydiab_train)\n",
    "\n",
    "ydiab_pred = clfdiab.predict(Xdiab_test)\n",
    "\n",
    "print(\"Train data accuracy:\",\"%.3f\" %(100*accuracy_score(y_true = ydiab_train, y_pred=clfdiab.predict(Xdiab_train))),\"%\")\n",
    "print(\"Test data accuracy:\",\"%.3f\" %(100*accuracy_score(y_true = ydiab_test, y_pred=ydiab_pred)),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = metrics.confusion_matrix(ydiab_test, ydiab_pred)\n",
    "#conf_matrix\n",
    "matrix_df = pd.DataFrame(conf_matrix) # Plot the result\n",
    "ax = plt.axes()\n",
    "sns.set(font_scale=1.3)\n",
    "sns.heatmap(matrix_df, annot=True, fmt=\"g\", ax=ax, cmap=\"viridis\") # Set axis titles\n",
    "ax.set_title('Confusion Matrix - Diabete detection')\n",
    "ax.set_xlabel(\"Predicted label\", fontsize =15)\n",
    "ax.set_ylabel(\"True Label\", fontsize=15) \n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
